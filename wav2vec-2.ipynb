{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nimport librosa, librosa.display\nfrom IPython.display import Audio\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom pydub import AudioSegment\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchvision import models, datasets, transforms","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:17:42.317602Z","iopub.execute_input":"2023-10-30T16:17:42.317875Z","iopub.status.idle":"2023-10-30T16:17:46.321502Z","shell.execute_reply.started":"2023-10-30T16:17:42.317851Z","shell.execute_reply":"2023-10-30T16:17:46.320530Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/common-voice/cv-valid-train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:04.637939Z","iopub.execute_input":"2023-10-30T16:18:04.638448Z","iopub.status.idle":"2023-10-30T16:18:05.207290Z","shell.execute_reply.started":"2023-10-30T16:18:04.638417Z","shell.execute_reply":"2023-10-30T16:18:05.206468Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:05.208850Z","iopub.execute_input":"2023-10-30T16:18:05.209231Z","iopub.status.idle":"2023-10-30T16:18:05.216456Z","shell.execute_reply.started":"2023-10-30T16:18:05.209191Z","shell.execute_reply":"2023-10-30T16:18:05.215564Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(195776, 8)"},"metadata":{}}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:05.217869Z","iopub.execute_input":"2023-10-30T16:18:05.218279Z","iopub.status.idle":"2023-10-30T16:18:05.241282Z","shell.execute_reply.started":"2023-10-30T16:18:05.218246Z","shell.execute_reply":"2023-10-30T16:18:05.240443Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                           filename  \\\n0  cv-valid-train/sample-000000.mp3   \n1  cv-valid-train/sample-000001.mp3   \n2  cv-valid-train/sample-000002.mp3   \n3  cv-valid-train/sample-000003.mp3   \n4  cv-valid-train/sample-000004.mp3   \n\n                                                text  up_votes  down_votes  \\\n0  learn to recognize omens and follow them the o...         1           0   \n1         everything in the universe evolved he said         1           0   \n2  you came so that you could learn about your dr...         1           0   \n3  so now i fear nothing because it was those ome...         1           0   \n4  if you start your emails with greetings let me...         3           2   \n\n   age gender accent  duration  \n0  NaN    NaN    NaN       NaN  \n1  NaN    NaN    NaN       NaN  \n2  NaN    NaN    NaN       NaN  \n3  NaN    NaN    NaN       NaN  \n4  NaN    NaN    NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>text</th>\n      <th>up_votes</th>\n      <th>down_votes</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>accent</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cv-valid-train/sample-000000.mp3</td>\n      <td>learn to recognize omens and follow them the o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cv-valid-train/sample-000001.mp3</td>\n      <td>everything in the universe evolved he said</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cv-valid-train/sample-000002.mp3</td>\n      <td>you came so that you could learn about your dr...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cv-valid-train/sample-000003.mp3</td>\n      <td>so now i fear nothing because it was those ome...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cv-valid-train/sample-000004.mp3</td>\n      <td>if you start your emails with greetings let me...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def show_cat(df):\n    print('teens', df.age.loc[df.age == 'teens'].count())\n    print('twenties', df.age.loc[df.age == 'twenties'].count())\n    print('thirties', df.age.loc[df.age == 'thirties'].count())\n    print('fourties', df.age.loc[df.age == 'fourties'].count())\n    print('fifties', df.age.loc[df.age == 'fifties'].count())\n    print('sixties', df.age.loc[df.age == 'sixties'].count())\n    print('seventies', df.age.loc[df.age == 'seventies'].count())\n    print('eighties', df.age.loc[df.age == 'eighties'].count())\n    return","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:06.102171Z","iopub.execute_input":"2023-10-30T16:18:06.102763Z","iopub.status.idle":"2023-10-30T16:18:06.109945Z","shell.execute_reply.started":"2023-10-30T16:18:06.102730Z","shell.execute_reply":"2023-10-30T16:18:06.109022Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# selecting the required fields\ntrain_age_df = train_df.loc[:,['filename','age']]","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:06.294884Z","iopub.execute_input":"2023-10-30T16:18:06.295166Z","iopub.status.idle":"2023-10-30T16:18:06.308184Z","shell.execute_reply.started":"2023-10-30T16:18:06.295140Z","shell.execute_reply":"2023-10-30T16:18:06.307149Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_age_df.fillna(0.0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:06.493852Z","iopub.execute_input":"2023-10-30T16:18:06.494146Z","iopub.status.idle":"2023-10-30T16:18:06.542885Z","shell.execute_reply.started":"2023-10-30T16:18:06.494120Z","shell.execute_reply":"2023-10-30T16:18:06.541984Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_age_df = train_age_df[train_age_df['age']!=0.0]","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:06.721567Z","iopub.execute_input":"2023-10-30T16:18:06.722220Z","iopub.status.idle":"2023-10-30T16:18:06.756030Z","shell.execute_reply.started":"2023-10-30T16:18:06.722190Z","shell.execute_reply":"2023-10-30T16:18:06.755196Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_age_df.loc[(train_age_df['age']=='eighties'),'age'] ='seventies'","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:06.901651Z","iopub.execute_input":"2023-10-30T16:18:06.901971Z","iopub.status.idle":"2023-10-30T16:18:06.919531Z","shell.execute_reply.started":"2023-10-30T16:18:06.901943Z","shell.execute_reply":"2023-10-30T16:18:06.918741Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# selecting 1800 samples from each category\nage_groups = train_age_df['age'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:07.121860Z","iopub.execute_input":"2023-10-30T16:18:07.122200Z","iopub.status.idle":"2023-10-30T16:18:07.134782Z","shell.execute_reply.started":"2023-10-30T16:18:07.122170Z","shell.execute_reply":"2023-10-30T16:18:07.133871Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"final_df = pd.DataFrame(columns= ['filepath', 'age'])\nfor age_grp in age_groups:\n    final_df = pd.concat([final_df, pd.DataFrame(train_age_df[train_age_df['age']==age_grp].sample(1800))], axis =0, ignore_index=True)\nfinal_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:07.341587Z","iopub.execute_input":"2023-10-30T16:18:07.341882Z","iopub.status.idle":"2023-10-30T16:18:07.510480Z","shell.execute_reply.started":"2023-10-30T16:18:07.341856Z","shell.execute_reply":"2023-10-30T16:18:07.509609Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(12600, 3)"},"metadata":{}}]},{"cell_type":"code","source":"show_cat(train_age_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:07.533340Z","iopub.execute_input":"2023-10-30T16:18:07.533627Z","iopub.status.idle":"2023-10-30T16:18:07.635195Z","shell.execute_reply.started":"2023-10-30T16:18:07.533602Z","shell.execute_reply":"2023-10-30T16:18:07.634373Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"teens 5441\ntwenties 23003\nthirties 18303\nfourties 11100\nfifties 9466\nsixties 4584\nseventies 1871\neighties 0\n","output_type":"stream"}]},{"cell_type":"code","source":"show_cat(final_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:07.754506Z","iopub.execute_input":"2023-10-30T16:18:07.754785Z","iopub.status.idle":"2023-10-30T16:18:07.779059Z","shell.execute_reply.started":"2023-10-30T16:18:07.754762Z","shell.execute_reply":"2023-10-30T16:18:07.778307Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"teens 1800\ntwenties 1800\nthirties 1800\nfourties 1800\nfifties 1800\nsixties 1800\nseventies 1800\neighties 0\n","output_type":"stream"}]},{"cell_type":"code","source":"def length_fixing(dataset):\n    \n    # each input is defined to have a 1s (1000ms) length\n    segment_length = 3000\n    \n    temp_df = pd.DataFrame(columns= ['filepath', 'age'])\n    \n    for _,sample in dataset.iterrows():\n    \n        audio_file_path =  '/kaggle/input/common-voice/cv-valid-train/' + sample['filename']\n        file_name = sample['filename'][-17:-4]\n\n        # loading audio using AudioSegment\n        audio = AudioSegment.from_file(audio_file_path, format=\"mp3\")\n\n        #print(len(audio))\n        if len(audio) < 2500:\n            continue\n        if len(audio) < 3000:\n            padding = AudioSegment.silent(duration=(segment_length - len(audio)))\n            audio += padding\n            # print(len(audio))\n            audio.export(\"/kaggle/working/\"+ file_name +\"segment_1.wav\", format=\"wav\")\n            new_record ={\n                    'filepath' : [\"/kaggle/working/\"+ file_name +\"segment_1.wav\"],\n                    'age' : [sample['age']]\n            }\n            temp_df = pd.concat([temp_df, pd.DataFrame(new_record)], ignore_index=True)    \n\n        segments = [audio[i:i+segment_length] for i in range(0, len(audio), segment_length)]\n\n        # padding the last segment to match the fixed length\n        last_seg_len = len(segments[-1])\n        if last_seg_len > 2500:\n            padding = AudioSegment.silent(duration=(segment_length - last_seg_len))\n            segments[-1] += padding\n        else:\n            segments = segments[:-1]\n        # print(len(segments[-1]))\n        for i,segment in enumerate(segments):\n            segment.export(\"/kaggle/working/\"+ file_name +f\"segment_{i}.wav\", format=\"wav\")\n            new_record ={\n                'filepath' : [\"/kaggle/working/\"+ file_name +f\"segment_{i}.wav\"],\n                'age' : [sample['age']]\n            }\n            temp_df = pd.concat([temp_df, pd.DataFrame(new_record)], ignore_index=True)  \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:07.982835Z","iopub.execute_input":"2023-10-30T16:18:07.983098Z","iopub.status.idle":"2023-10-30T16:18:07.994367Z","shell.execute_reply.started":"2023-10-30T16:18:07.983075Z","shell.execute_reply":"2023-10-30T16:18:07.993389Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df_seg = length_fixing(final_df)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:18:08.221517Z","iopub.execute_input":"2023-10-30T16:18:08.221806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_cat(train_df_seg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_cat(train_df_seg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Val Split","metadata":{}},{"cell_type":"code","source":"train_df = pd.DataFrame(columns= ['filepath', 'age'])\nfor age_grp in age_groups:\n    train_df = pd.concat([train_df, pd.DataFrame(train_df_seg[train_df_seg['age']==age_grp].sample(1440))], axis =0, ignore_index=True)\ntrain_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = train_df_seg.merge(train_df, how='left', indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\ntest_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = pd.DataFrame(columns= ['filepath', 'age'])\nfor age_grp in age_groups:\n    val_df = pd.concat([val_df, pd.DataFrame(test_df[test_df['age']==age_grp].sample(360))], axis =0, ignore_index=True)\nval_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(dataset):\n    \n    temp_feature_list = []\n    temp_label_list = []\n    \n    # loop through th erows of dataframe\n    \n    for _, row in dataset.iterrows():\n        \n        file_path = row['filepath']\n        label = row['age']\n        #print('filepath:', file_path)\n        #print('label:', label)\n        # loading the audion file\n        audio, sr = librosa.load(file_path, sr=28000)\n        if len(audio) < sr*3:\n            audio = librosa.util.pad_center(audio, size=sr*3)\n        # print(len(audio))\n        MFCCs = librosa.feature.mfcc(y=audio[:sr*3],sr=sr, n_fft=1024,hop_length=128,n_mfcc=128)\n        #print(MFCCs_.shape)\n        # log spectro of the MFCCs\n        MFCCs_ = librosa.amplitude_to_db(MFCCs)\n        temp_feature_list.append(MFCCs_)\n        temp_label_list.append(label)\n        \n    return np.array(temp_feature_list), np.array(temp_label_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, Y_train = extract_features(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, Y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val, Y_val = extract_features(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val.shape, Y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Data Loaders","metadata":{}},{"cell_type":"code","source":"X_test = X_test.view(2520,-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:18.424934Z","iopub.execute_input":"2023-10-30T14:10:18.425200Z","iopub.status.idle":"2023-10-30T14:10:18.428886Z","shell.execute_reply.started":"2023-10-30T14:10:18.425179Z","shell.execute_reply":"2023-10-30T14:10:18.428121Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.view(10080,-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:18.108973Z","iopub.execute_input":"2023-10-30T14:10:18.109676Z","iopub.status.idle":"2023-10-30T14:10:18.121858Z","shell.execute_reply.started":"2023-10-30T14:10:18.109650Z","shell.execute_reply":"2023-10-30T14:10:18.121246Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# converting the target class into one-hot-encoded vectors\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\n\nY_train_lb = lb.fit_transform(Y_train)\nY_val_lb = lb.fit_transform(Y_val)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:13.665257Z","iopub.execute_input":"2023-10-30T14:10:13.665565Z","iopub.status.idle":"2023-10-30T14:10:13.679629Z","shell.execute_reply.started":"2023-10-30T14:10:13.665538Z","shell.execute_reply":"2023-10-30T14:10:13.679064Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Y_train_lb.shape, Y_val_lb.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:14.006410Z","iopub.execute_input":"2023-10-30T14:10:14.006856Z","iopub.status.idle":"2023-10-30T14:10:14.011994Z","shell.execute_reply.started":"2023-10-30T14:10:14.006834Z","shell.execute_reply":"2023-10-30T14:10:14.011238Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"((10080, 7), (2520, 7))"},"metadata":{}}]},{"cell_type":"code","source":"X_train = scaler.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:20.885197Z","iopub.execute_input":"2023-10-30T14:10:20.885476Z","iopub.status.idle":"2023-10-30T14:10:25.064279Z","shell.execute_reply.started":"2023-10-30T14:10:20.885455Z","shell.execute_reply":"2023-10-30T14:10:25.062184Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:25.067195Z","iopub.execute_input":"2023-10-30T14:10:25.067793Z","iopub.status.idle":"2023-10-30T14:10:26.124294Z","shell.execute_reply.started":"2023-10-30T14:10:25.067764Z","shell.execute_reply":"2023-10-30T14:10:26.123357Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(X_train, dtype=torch.float32)\nY_train = torch.tensor(Y_train_lb, dtype=torch.float32)\nX_test = torch.tensor(X_val, dtype=torch.float32)\nY_test = torch.tensor(Y_val_lb, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:17.185376Z","iopub.execute_input":"2023-10-30T14:10:17.185883Z","iopub.status.idle":"2023-10-30T14:10:18.107762Z","shell.execute_reply.started":"2023-10-30T14:10:17.185857Z","shell.execute_reply":"2023-10-30T14:10:18.107085Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#X_train_double = torch.as_tensor(X_train, dtype=torch.double)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:12:33.326628Z","iopub.execute_input":"2023-10-30T14:12:33.326994Z","iopub.status.idle":"2023-10-30T14:12:33.331277Z","shell.execute_reply.started":"2023-10-30T14:12:33.326968Z","shell.execute_reply":"2023-10-30T14:12:33.330502Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#X_test_double = torch.as_tensor(X_test, dtype=torch.double)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:37:11.387054Z","iopub.execute_input":"2023-10-30T14:37:11.387369Z","iopub.status.idle":"2023-10-30T14:37:11.391963Z","shell.execute_reply.started":"2023-10-30T14:37:11.387347Z","shell.execute_reply":"2023-10-30T14:37:11.391072Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#tensor_double","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:10:58.716926Z","iopub.execute_input":"2023-10-30T14:10:58.717251Z","iopub.status.idle":"2023-10-30T14:10:58.782500Z","shell.execute_reply.started":"2023-10-30T14:10:58.717227Z","shell.execute_reply":"2023-10-30T14:10:58.781709Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.8439,  0.8364,  0.8346,  ..., -0.9276,  0.0349,  0.0226],\n        [ 0.9250,  0.9250,  0.9250,  ..., -0.2030, -0.1443, -0.7518],\n        [ 0.9264,  0.9264,  0.9264,  ..., -0.0344,  0.0128, -0.5601],\n        ...,\n        [ 0.9265,  0.9265,  0.9265,  ..., -0.1544, -0.4243, -0.3394],\n        [ 0.9208,  0.9208,  0.9208,  ...,  0.2419,  0.0352, -0.1892],\n        [ 0.9455,  0.9455,  0.9455,  ..., -0.8526, -0.7845, -0.8432]],\n       dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"code","source":"train_loader = DataLoader(TensorDataset(X_train_double,Y_train),batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_test_double,Y_test), batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:40:16.685648Z","iopub.execute_input":"2023-10-30T14:40:16.686532Z","iopub.status.idle":"2023-10-30T14:40:16.693233Z","shell.execute_reply.started":"2023-10-30T14:40:16.686508Z","shell.execute_reply":"2023-10-30T14:40:16.692466Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_loader","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:11:09.860312Z","iopub.execute_input":"2023-10-30T12:11:09.860636Z","iopub.status.idle":"2023-10-30T12:11:09.866224Z","shell.execute_reply.started":"2023-10-30T12:11:09.860614Z","shell.execute_reply":"2023-10-30T12:11:09.865362Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7dbe154b7a90>"},"metadata":{}}]},{"cell_type":"code","source":"# freeing up memory using garbage collector\nimport gc\n\n#X_val = None\n#X_train = None\n#X_test = None\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:04:01.352080Z","iopub.execute_input":"2023-10-30T12:04:01.352447Z","iopub.status.idle":"2023-10-30T12:04:01.541967Z","shell.execute_reply.started":"2023-10-30T12:04:01.352423Z","shell.execute_reply":"2023-10-30T12:04:01.541020Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"1663"},"metadata":{}}]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"# importing wav2vec base model\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Load the pre-trained Wav2Vec model\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base')\n\n# Modify the model head for classification (replace the last layer)\n# Here is a simple example, you may need to adapt this according to your requirements\n#","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:40:23.364881Z","iopub.execute_input":"2023-10-30T14:40:23.365250Z","iopub.status.idle":"2023-10-30T14:40:36.150478Z","shell.execute_reply.started":"2023-10-30T14:40:23.365226Z","shell.execute_reply":"2023-10-30T14:40:36.149574Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6660d07dd5f47299b6933d46d266106"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a116363523b4971aea330520b14c677"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.hidden_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = nn.Sequential(# 102,256,3--> 3channels\n    \n    nn.Conv2d(3, 32,  kernel_size=3, stride=1, padding=1),#102,256\n    nn.MaxPool2d(2, stride=2), # 51,128\n    \n    nn.Conv2d(32, 64,  kernel_size=3, stride=1, padding=1),#51,128\n    nn.MaxPool2d((3,2), stride=2), # 25,64\n    \n    nn.Conv2d(64, 32,  kernel_size=3, stride=1, padding=1),#25,64\n    nn.MaxPool2d((3,2), stride=2), # 12,32\n    \n    nn.Flatten(),\n    \n    nn.Linear(in_features= 32*12*32, out_features=4096),\n    nn.ReLU(),\n    nn.Linear(in_features=4096, out_features=1024),\n    nn.ReLU(),\n    nn.Linear(1024, 7)  # Output 7 classes\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resnet Model Pretrained","metadata":{}},{"cell_type":"code","source":"resnet = models.resnet50(pretrained=True)\n\n# Freeze the layers except the final fully connected layers\nfor param in resnet.parameters():\n    param.requires_grad = False\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:05.184568Z","iopub.execute_input":"2023-10-30T14:41:05.185364Z","iopub.status.idle":"2023-10-30T14:41:06.800802Z","shell.execute_reply.started":"2023-10-30T14:41:05.185339Z","shell.execute_reply":"2023-10-30T14:41:06.799879Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:01<00:00, 90.4MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"resnet.fc = nn.Linear(resnet.fc.in_features, 7)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:06.802383Z","iopub.execute_input":"2023-10-30T14:41:06.802721Z","iopub.status.idle":"2023-10-30T14:41:06.807356Z","shell.execute_reply.started":"2023-10-30T14:41:06.802683Z","shell.execute_reply":"2023-10-30T14:41:06.806561Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the optimizers and loss functions\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=0.00003)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:48.100883Z","iopub.execute_input":"2023-10-30T14:41:48.101505Z","iopub.status.idle":"2023-10-30T14:41:48.107405Z","shell.execute_reply.started":"2023-10-30T14:41:48.101480Z","shell.execute_reply":"2023-10-30T14:41:48.106556Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"def evaluate(model_1,model_2, iterator, criterion):\n    model_2.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in iterator:\n            src, trg = batch\n            batch_size = src.shape[0]\n            src = src.view(batch_size,-1)\n            logits = model_1.wav2vec2(src)\n            input_2 = logits.last_hidden_state.view(batch_size,3,102,256)\n            output = model_2(input_2)\n            loss = criterion(output, trg)\n            total_loss += loss.item()\n    return total_loss / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:19.185046Z","iopub.execute_input":"2023-10-30T14:41:19.185368Z","iopub.status.idle":"2023-10-30T14:41:19.191383Z","shell.execute_reply.started":"2023-10-30T14:41:19.185344Z","shell.execute_reply":"2023-10-30T14:41:19.190449Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Training loop\nmodel_2.train()\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        src, trg = batch\n        batch_size = src.shape[0]\n        src = src.view(batch_size,-1)\n        #print(src.shape)\n        optimizer.zero_grad()\n        logits = 0\n        with torch.no_grad():\n            logits = model.wav2vec2(src)\n        #print(logits)\n        print(logits.last_hidden_state.shape)\n        input_2 = logits.last_hidden_state.view(batch_size,3,102,256)\n        output = model_2(input_2)\n        loss = criterion(output, trg)  # Adapt loss computation based on your setup\n        loss.backward()\n        optimizer.step()\n        print('loss:',loss)\n        \n    val_loss = evaluate(model, model_2, val_loader, criterion)\n    print(f'\\tValidation Loss: {val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:22.904643Z","iopub.execute_input":"2023-10-30T14:41:22.904948Z","iopub.status.idle":"2023-10-30T14:41:23.204807Z","shell.execute_reply.started":"2023-10-30T14:41:22.904924Z","shell.execute_reply":"2023-10-30T14:41:23.203996Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n","\u001b[0;31mNameError\u001b[0m: name 'model_2' is not defined"],"ename":"NameError","evalue":"name 'model_2' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training ResNet + Wav2vec","metadata":{}},{"cell_type":"code","source":"# defining the optimizers and loss functions\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=0.00003)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:55.365092Z","iopub.execute_input":"2023-10-30T14:41:55.365954Z","iopub.status.idle":"2023-10-30T14:41:55.370785Z","shell.execute_reply.started":"2023-10-30T14:41:55.365921Z","shell.execute_reply":"2023-10-30T14:41:55.370203Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:41:57.044649Z","iopub.execute_input":"2023-10-30T14:41:57.044979Z","iopub.status.idle":"2023-10-30T14:41:57.049453Z","shell.execute_reply.started":"2023-10-30T14:41:57.044953Z","shell.execute_reply":"2023-10-30T14:41:57.048657Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet = resnet.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_2(model_1,model_2, iterator, criterion):\n    model_2.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in iterator:\n            src, trg = batch\n            src = src.to(device)\n            trg = trg.to(device)\n            logits = model_1.wav2vec2(src)\n            input_2 = logits.last_hidden_stateview(batch_size,3,262,256)\n            output = model_2(input_2)\n            loss = criterion(output, trg)\n            total_loss += loss.item()\n    return total_loss / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T15:26:46.985446Z","iopub.execute_input":"2023-10-30T15:26:46.985844Z","iopub.status.idle":"2023-10-30T15:26:47.027014Z","shell.execute_reply.started":"2023-10-30T15:26:46.985813Z","shell.execute_reply":"2023-10-30T15:26:47.025802Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Training loop\nresnet.train()\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        src, trg = batch\n        src = src.to(device)\n        trg = trg.to(device)\n        optimizer.zero_grad()\n        logits = 0\n        print(type(src))\n        with torch.no_grad():\n            logits = model.wav2vec2(src)\n        #print(logits)\n        #print(logits.last_hidden_state.shape)\n        input_2 = logits.last_hidden_state.view(batch_size,3,262,256)\n        #print(input_2.shape)\n        output = resnet(input_2)\n        loss = criterion(output, trg)  # Adapt loss computation based on your setup\n        loss.backward()\n        optimizer.step()\n        print('loss:',loss)\n        \n    val_loss = evaluate_2(model, resnet, val_loader, criterion)\n    print(f'\\tValidation Loss: {val_loss:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model","metadata":{}},{"cell_type":"code","source":"import pickle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(CNN_model, '/kaggle/working/Age_wav2vec_CNN.pth')\n\nwith open('/kaggle/working/Age_wav2vec_CNN.pkl', 'wb') as file:\n    pickle.dump(CNN_model, file)\n    \ntorch.save(CNN_model, '/kaggle/working/Age_wav2vec_CNN.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading Models","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink","metadata":{"execution":{"iopub.status.busy":"2023-10-30T10:24:00.197547Z","iopub.execute_input":"2023-10-30T10:24:00.197881Z","iopub.status.idle":"2023-10-30T10:24:00.201740Z","shell.execute_reply.started":"2023-10-30T10:24:00.197860Z","shell.execute_reply":"2023-10-30T10:24:00.200746Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"FileLink(r'Age_wav2vec_CNN.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'Age_wav2vec_CNN.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'Age_wav2vec_CNN.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T10:32:42.902970Z","iopub.execute_input":"2023-10-30T10:32:42.903302Z","iopub.status.idle":"2023-10-30T10:32:42.908744Z","shell.execute_reply.started":"2023-10-30T10:32:42.903280Z","shell.execute_reply":"2023-10-30T10:32:42.907893Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}